%% See http://yihui.name/knitr/demo/child/ for documentation on the parent/child document system of knitr


\Sexpr{set_parent('NimbleUserManual.Rnw')}

<<echo=FALSE>>=
require(nimble)
@ 

\chapter{Other algorithms provided by NIMBLE}
\label{cha:algos-provided}


In v\ver, the NIMBLE algorithm library is fairly limited beyond MCMC.
It includes some basic utilities for calculating and simulating sets
of nodes.  And it includes a couple of algorithms, particle filters
and MCEM, that illustrate the kind of programming with models that can
be done with NIMBLE.

\section{Basic Utilities}
\label{sec:basic-utils}

\subsection{\cd{simNodes}, \cd{calcNodes}, and \cd{getLogProbs}}
\label{sec:cdsimn-cdcalcn-cdget}



  \cd{simNodes}, \cd{calcNodes} and \cd{getLogProb} are basic nimbleFunctions that simulate, calculate, or get the log probabilities
  (densities), respectively, of the same set of nodes each time they
  are called.  Each of these 
takes a model and a character string of node names 
  as inputs. If \cd{nodes} is left blank, then all the nodes of the model
  are used. 
  
  For \cd{simNodes}, the nodes provided will be topologically sorted to 
  simulate in the correct order. For \cd{calcNodes} and \cd{getLogProb},
  the nodes will be sorted and dependent nodes will be included.  Recall that
 the calculations must be up to date (from a calculate call) for \cd{getLogProb} 
  to return the values you are probably looking for.
  
<<Basic_Utils_Algs>>=
simpleModelCode <- nimbleCode({
  for(i in 1:4){
    x[i] ~ dnorm(0,1)
    y[i] ~ dnorm(x[i], 1) #y depends on x
    z[i] ~ dnorm(y[i], 1) #z depends on y
    #z conditionally independent of x
  }
})

simpleModel <- nimbleModel(simpleModelCode, check = FALSE)
cSimpleModel <- compileNimble(simpleModel)

#simulates all the x's and y's
rSimXY <- simNodes(simpleModel, nodes = c('x', 'y') ) 

#calls calculate on x and its dependents (y, but not z)
rCalcXDep <- calcNodes(simpleModel, nodes = 'x')

#calls getLogProb on x's and y's
rGetLogProbXDep <- getLogProbNodes(simpleModel,
                  nodes = 'x')

#compiling the functions
cSimXY <- compileNimble(rSimXY, project = simpleModel)
cCalcXDep <- compileNimble(rCalcXDep, project = simpleModel)
cGetLogProbXDep <- compileNimble(rGetLogProbXDep,
                           project = simpleModel)

cSimpleModel$x
cSimpleModel$y
#simulating x and y
cSimXY$run()
cSimpleModel$x
cSimpleModel$y

cCalcXDep$run()

#Gives correct answer because logProbs
#updated by 'calculate' after simulation
cGetLogProbXDep$run()

cSimXY$run()

#Gives old answer because logProbs
#not updated after 'simulate'
cGetLogProbXDep$run()
cCalcXDep$run()
@


\subsection{\cd{simNodesMV}, \cd{calcNodesMV}, and \cd{getLogProbsMV}}

There is a similar trio of nimbleFunctions that does each job
repeatedly for different rows of a modelValues object.  For example,
\cd{simNodesMV} will simulate in the model multiple times and record
each simulation in a row of its modelValues.  \cd{calcNodesMV} and
\cd{getLogProbsMV} iterate over the rows of a modelValues, copy the
nodes into the model, and then do their job of calculating or
collecting log probabilities (densities), respectively.  Each of these
returns a numeric vector with the summed log probabilities of the
chosen nodes from each
each row. \cd{calcNodesMV} will 
  save the log probabilities back into the modelValues object if
  \cd{saveLP == TRUE}, a run-time argument. 

Here are some examples:

<<Basic_Utils_MV>>=
mv <- modelValues(simpleModel)
rSimManyXY <- simNodesMV(simpleModel, nodes = c('x', 'y'), mv = mv)
rCalcManyXDeps <- calcNodesMV(simpleModel, nodes = 'x', mv = mv)
rGetLogProbMany <- getLogProbNodesMV(simpleModel,
                    nodes = 'x', mv = mv)

cSimManyXY <- compileNimble(rSimManyXY, project = simpleModel)
cCalcManyXDeps <- compileNimble(rCalcManyXDeps, project = simpleModel)
cGetLogProbMany <- compileNimble(rGetLogProbMany, project = simpleModel)

cSimManyXY$run(m = 5) # simulating 5 times
cCalcManyXDeps$run(saveLP = TRUE) # calculating 
cGetLogProbMany$run() #
@
  
  

\section{Particle Filters}

\subsection{Filtering Algorithms}

  NIMBLE includes algorithms for four different types of particle filters, which can be used to sample from the latent states and approximate the log likelihood of a state space model.  The particle filters currently implemented in Nimble are the bootstrap filter, the auxiliary particle filter, the Liu-West filter, and the ensemble Kalman filter, which can be built, respectively, with calls to \cd{buildBootF}, \cd{buildAuxF}, \cd{buildLWF}, and \cd{buildENKF}.  Each particle filter requires setup arguments  \cd{model} and \cd{nodes}, which is a character vector specifying latent model nodes.  In addition, each particle filter can be customized using a \cd{control} list argument.  Details on the control options and specifics of the filtering algorithms can be found in R's help pages.  
  
  Once built, each filter can be run by specifying the number of particles.  Each filter has a model values object named \cd{mvEWSamp} which is populated with equally-weighted samples from the posterior distribution of the latent states (and in the case of the Liu =West filter, the posterior distribution of the top level parameters as well) as the filter is run.  The bootstrap, auxiliary, and Liu-West filters also have another model values object, \cd{mvWSamp}, which has unequally-weighted samples from the posterior distribution of the latent states, along with weights for each particle.   In addition, the bootstrap and auxiliary particle filters return estimates of the log likelihood of the given state space model.
  
 We first create a linear state-space model to use as an example for our particle filter algorithms. 
  <<particle_Filter_Chunk, results = "hide">>=
  # Building a simple linear state-space model. 
  # x is latent space, y is observed data
  timeModelCode <- nimbleCode({
  x[1] ~ dnorm(mu_0, 1)
  y[1] ~ dnorm(x[1], 1)
  for(i in 2:t){
  x[i] ~ dnorm(x[i-1] * a + b, 1)
  y[i] ~ dnorm(x[i] * c, 1)
  }
  
  a ~ dunif(0, 1)
  b ~ dnorm(0, 1)
  c ~ dnorm(1,1)
  mu_0 ~ dnorm(0, 1)
  })
  
  #simulate some data
  t = 25; mu_0 = 1
  x = rnorm(1 ,mu_0, 1)
  y = rnorm(1, x, 1)
  a = 0.5; b = 1; c = 1
  for(i in 2:t){
  x[i] = rnorm(1, x[i-1] * a + b, 1)
  y[i] = rnorm(1, x[i] * c, 1)
  }
  
  #build the model
  rTimeModel <- nimbleModel(timeModelCode, constants = list(t = t), 
  data = list(y = y), check = FALSE )

  #Set parameter values and compile the model
  rTimeModel$a = 0.5
  rTimeModel$b = 1
  rTimeModel$c = 1
  rTimeModel$mu_0 = 1

  cTimeModel <- compileNimble(rTimeModel)


  @
  Here is an example of building and running the bootstrap filter.  Additional information about the bootstrap filter can be found with \cd{help(buildBootF)}.
  
<<boot_Filter_Chunk>>=

  #Build bootstrap filter
  rBootF <- buildBootF(rTimeModel, "x", 
                       control = list(thresh = 0.8, saveAll = T, smoothing = F))
  #Compile filter   
  cBootF = compileNimble(rBootF,project = rTimeModel)
  #Set number of particles
  parNum <- 5000
  #Run bootstrap filter, which returns estimate of model log-likelihood
  bootLLEst <- cBootF$run(parNum)
@
  Next, we provide an example of building and running the auxiliary particle filter.  Additional information about the auxiliary particle filter can be found with \cd{help(buildAuxF)}.  Note that a filter cannot be built on a model that already has a filter specialized to it, so we create a new copy of our state space model first
  
<<aux_Filter_Chunk, results = "hide">>=

  #Copy our state-space model for use with the auxiliary filter
  auxTimeModel <- rTimeModel$newModel(replicate = T)
  compileNimble(auxTimeModel)
  #Build auxiliary filter
  rAuxF <- buildAuxF(auxTimeModel, "x", 
                     control = list(thresh = 0.5, saveAll = T))
  #Compile filter   
  cAuxF = compileNimble(rAuxF,project = auxTimeModel)
  #Run auxliary filter, which returns estimate of model log-likelihood
  auxLLEst <- cAuxF$run(parNum)
@
  Below we give an example of building and running the Liu-West filter, which can sample from the posterior distribution of top-level parameters as well as latent states. Additional information can be found with \cd{help(buildLWF)}.

<<lw_Filter_Chunk, results = "hide">>=

  #Copy model
  LWTimeModel <- rTimeModel$newModel(replicate = T)
  compileNimble(LWTimeModel)
  #Build Liu-West filter, also 
  #specifying which top level parameters to estimate
  rLWF <- buildLWF(LWTimeModel, "x",
                   control = list(params = c("a", "b", "c"),
                                  saveAll = F))     
  #Compile filter   
  cLWF = compileNimble(rLWF,project = LWTimeModel)
  #Run Liu-West filter
  cLWF$run(parNum)
@
  Below we give an example of building and running the ensemble Kalman filter, which can sample from the posterior distribution of latent states. Additional information can be found with \cd{help(buildAuxF)}.

<<ENKF_Filter_Chunk, results = "hide">>=
  #Copy model
  ENKFTimeModel <- rTimeModel$newModel(replicate = T)
  compileNimble(ENKFTimeModel)
  #Build and compile ensemble Kalman filter
  rENKF <- buildENKF(ENKFTimeModel, "x",
                     control = list(saveAll = F))  
  cENKF = compileNimble(rENKF,project = ENKFTimeModel)
  #Run ensemble Kalman filter
  cENKF$run(parNum)
  @
  
  Once each filter has been run, we can extract samples from the posterior distribution of our latent states as follows:
  
  <<particle_Filter_Samples, eval=FALSE>>=
  #Equally-weighted samples (available from all filters)
  bootEWSamp <- as.matrix(cBootF$mvEWSamp)
  auxEWSamp <- as.matrix(cAuxF$mvEWSamp)
  LWFEWSamp <- as.matrix(cLWF$mvEWSamp)
  ENKFEWSamp <- as.matrix(cENKF$mvEWSamp)
  
  #Unequally-weighted samples, along with weights (available 
  #from bootstrap, auxiliary, and Liu and West filters)
  bootWSamp <- as.matrix(cBootF$mvWSamp, 'x')
  bootWts <- as.matrix(cBootF$mvWSamp, 'wts')
  auxWSamp <-  as.matrix(xAuxF$mvWSamp, 'x')
  auxWts <- as.matrix(cAuxF$mvWSamp, 'wts')
  
  #Liu and West filter also returns samples 
  #from posterior distribution of top-level parameters:
  aEWSamp <- as.matrix(cLWF$mvEWSamp, 'a')
  @

    
\subsection{Particle MCMC}
\label{sec:particle-mcmc}

In addition to our four particle filters, NIMBLE also has a particle MCMC sampler implemented, which samples top-level parameters, using a bootstrap filter to obtain estimates of the likelihood of a model for use in a Metropolis-Hastings MCMC step.  The PMCMC sampler uses a multivariate normal proposal distribution, and currently can only be used for blocks of top-level parameters.  Our PMCMC sampler also includes an optional algorithm to estimate the optimal number of particles to use in the particle filter at each iteration, based on a trade off between computational time and efficiency.  The PMCMC sampler can be specified with a call to \cd{addSampler} with \cd{type = 'RW\_PFilter'}, a syntax similar to the other MCMC samplers listed in \ref{sec:samplers-provided}.

The \nm{RW\_PF} sampler can be customized using the \cd{control} list argument to set the initial proposal covariance, the adaptive properties of the sampler, and options for the particle filter algorithm to be run.  In addition, setting the \cd{optimizeM} control list option to \cd{TRUE} will allow the sampler to estimate the optimal number of particles for the bootstrap filter.   See \cd{help(samplers)} for details. The MCMC configuration for the \cd{timeModel} in the previous section will serve as an example for the use of our PMCMC sampler.  Here we use the identity matrix as our proposal covariance matrix. 

  <<pmcmc_Chunk, results = "hide">>=
  timespec <- configureMCMC(rTimeModel)   # default MCMC configuration
  
  # Add random walk pmcmc sampler with particle number optimization.
  timespec$addSampler(target = c('a', 'b', 'c', 'mu_0'), type = 'RW_PFilter',
  control = list(propCov= diag(4),
				 adaptScaleOnly=F,
				 latents = 'x',
				 optimizeM = TRUE
				 ))
	@
  

    
\section{Monte Carlo Expectation Maximization (MCEM)}

   Suppose we have a model with missing data (or a layer of latent
  variables that can be treated as missing data) and we would like to
  maximize the marginal likelihood of the model, integrating over the
  missing data. A brute-force method for doing this is MCEM. This is
  an EM algorithm in which the missing data are simulated via Monte
  Carlo (often MCMC, when the full conditional distributions cannot be
  directly sampled from) at each iteration.  MCEM can be slow, and
  there are other methods for maximizing marginal likelihoods that can
  be implemented in NIMBLE.  The reason we started with MCEM is to
  explore the flexibility of NIMBLE and illustrate the combination of
  R and NIMBLE involved, with R managing the highest-level processing
  of the algorithm and calling nimbleFunctions for computations.  NIMBLE provides two MCEM algorithms:
  an algorithm with a user-specified number of iterations and MCMC sample size (\cd{buildMCEM}), and an Ascent-based MCEM algorithm which  automatically determines when the algorithm has converged by examining 
  the size of the changes in the likelihood between each iteration (\cd{buildAscentMCEM}).
  
  
  We will revisit the \nm{pump} example to illustrate the use of
  NIMBLE's MCEM algorithms.
  
 %% newPump didn't exist so I'm creating it in some non-echoed code.
<<echo=FALSE>>=
pumpCode <- nimbleCode({ 
  for (i in 1:N){
      theta[i] ~ dgamma(alpha,beta)
      lambda[i] <- theta[i]*t[i]
      x[i] ~ dpois(lambda[i])
  }
  alpha ~ dexp(1.0)
  beta ~ dgamma(0.1,1.0)
})

pumpConsts <- list(N = 10,
                   t = c(94.3, 15.7, 62.9, 126, 5.24,
                       31.4, 1.05, 1.05, 2.1, 10.5))

pumpData <- list(x = c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22))

pumpInits <- list(alpha = 1, beta = 1,
                  theta = rep(0.1, pumpConsts$N))



@   
% note that code below doesn't get evaluated because the buildMCEM functions do not work on models which
% have mcmc's already built for them.  Since building an MCEM requires a MCMC to be built simultaneously,
% we cant have both mcems at the same time.
<<build-MCEM, results = "hide">>=

#rebuild and recompile the pump model
newPump <- nimbleModel(code = pumpCode, name = 'pump',
                       constants = pumpConsts,
                       data = pumpData, 
                       inits = pumpInits,
                       check = FALSE)

compileNimble(newPump)

pumpMCEM <- buildMCEM(model = newPump,
                      latentNodes = 'theta',
                      burnIn = 100,
                      mcmcControl = list(adaptInterval = 20),
                      boxConstraints = list( list( c('alpha', 'beta'), 
                          limits = c(0, Inf) ) ), 
                      buffer = 1e-6)

#make a copy of the pump model for use with the ascentMCEM algoritm
ascentPump <- newPump$newModel(replicate=T)                      
compileNimble(ascentPump)
#build an MCEM algorithm with Ascent-based convergence criterion
pumpAscentMCEM <- buildAscentMCEM(model = ascentPump,
                                  latentNodes = 'theta',
                                  burnIn = 100,
                                  mcmcControl = list(adaptInterval = 20),
                                  boxConstraints =
                                    list( list( c('alpha', 'beta'), 
                                     limits = c(0, Inf) ) ), 
                                  buffer = 1e-6)

@

%  When building the MCEM algorithm, the arguments supplied are \cd{model}, \cd{latentNodes}, \cd{burnIn}, \cd{mcmcControl}, \cd{boxConstraints} and \cd{buffer}. 
  Here \cd{newPump} was created just like \cd{pump} in Section \ref{sec:creating-model}. The first argument to each algorithm, \cd{model}, is a NIMBLE model, which can be
  either the uncompiled or compiled version. At the moment, the model provided cannot be part of another MCMC sampler.  Both the user-specified and ascent-based MCEM algorithms share a number of control options:
 
 
  The \cd{latentNodes} argument should indicate the nodes that will be
  integrated over (sampled via MCMC), rather than
  maximized.  These
  nodes must be stochastic, not deterministic! \cd{latentNodes} will
  be expanded as described in Section \ref{sec:arbitr-coll-nodes}:
  e.g., either \cd{latentNodes = `x'} or \cd{latentNodes = c(`x[1]',
    `x[2]')} will treat \cd{x[1]} and \cd{x[2]} as latent nodes if
  \cd{x} is a vector of two values. All other non-data nodes will be
  maximized over. Note that \cd{latentNodes} can include discrete nodes,
  but the nodes to be maximized cannot.  

 The \cd{burnIn} argument indicates the number of samples from the MCMC for the E-step that should be discarded when computing the expected likelihood in the M-step. Note that \cd{burnIn} can be set to values lower than in standard MCMC computations, as each iteration will start off where the last left off. 
  
The  \cd{mcmcControl} argument will be passed to \cd{configureMCMC} to define the MCMC to be used.

 Each MCEM algorithm allows for box constraints on the nodes that will
 be optimized, specified via the \cd{boxConstraints} argument. This is
 highly recommended for nodes that have zero density on parts of the
 real line\footnote{Currently NIMBLE does not determine this automatically.}.
 Each constraint given should be a list
 in which the first element is the names of the nodes or variables
 that the constraint will be applied to and the second element is a
 vector of length 2, in which the first value is the lower limit and
 the second is the upper limit.  Values of \cd{Inf} and \cd{-Inf} are allowed. If a node is not listed, it will be assumed that there are no constraints. These arguments are passed as \cd{lower} and \cd{upper} to R's \cd{optim} function, using \cd{method = `L-BFGS-B'}) 

 The value of the  \cd{buffer} argument shrinks the
 \cd{boxConstraints} by this amount.  This can help protect against
 non-finite values occurring when a parameter is on its boundary value. 
  
  In addition, the ascent-based MCEM has some extra control options that can be used to further tune the convergence criterion.  See \cd{help(buildAscentMCEM)} for more information.  
  
Once an MCEM has been built for the model of interest using either \cd{buildMCEM} or \cd{buildAscentMCEM}, it can be run as follows.

<<run-MCEM, eval=FALSE>>=
pumpMCEM(maxit = 20, m1 = 250, m2 = 500)
pumpAscentMCEM(initM = 1000)
@ 

For the user-specified MCEM algorithm, there are three run-time arguments:
 
The \cd{maxit} argument is the number of total iterations to run the algorithm. More advanced MCEM algorithms have a stopping criteria based on computing the MCMC error. Our current draft implementation of the algorithm merely runs \cd{maxit} iterations and then terminates. 
  
  Halfway through the algorithm, the sample size used for the E-step
  switches from \cd{m1} to \cd{m2}.  This provides smaller MCMC error
  as the algorithm converges. If \cd{m1} or \cd{m2} is less than or equal to \cd{burnIn} as defined in \cd{build\_MCEM}, the user-specified MCEM algorithm will immediately quit.
    When using the user-specified MCEM algorithm, we suggest first starting with small values of \cd{m1} and \cd{m2} to get an estimate of how long the algorithm will take for larger MCMC samples. The speed of the algorithm will be linear in \cd{m2} (assuming that $m2 > m1$ as intended)
  
The ascent-based MCEM algorithm has only one run-time argument, \cd{initM}, which is the number of MCMC iterations to use when the algorithm is initialized.

\section{Normalizing Constant Estimation}

 NIMBLE has an algorithm to estimate the normalizing constant of a model using importance sampling.  This algorithm first runs MCMC to get an approximation to the posterior distribution. See \cd{help(getNormConst)} for additional information on building the algorithm.  Note that the estimate of the normalizing constant produced by this algorithm is very sensitive to the accuracy of the normal approximation.  For this reason, it is important to ensure that the MCMC used to obtain posterior samples has converged.  We will again use the \nm{pump} example to illustrate NIMBLE's normalizing constant estimation algorithm. 

<<getNormConst>>=
newPump2 <- newPump$newModel(replicate = TRUE)
compileNimble(newPump2)
pumpNormConst <- getNormConst(model = newPump2,
                              burnIn = 1000,
                              mcmcControl = list(adaptInterval = 100))
cConstEst <- compileNimble(pumpNormConst, project = newPump2)
constEst <- cConstEst$run(5000, 5000)
@   
