%% See http://yihui.name/knitr/demo/child/ for documentation on the parent/child document system of knitr

\Sexpr{set_parent('NimbleUserManual.Rnw')}

<<echo=FALSE>>=
require(nimble)
@ 

\chapter{Other algorithms provided by NIMBLE}
\label{cha:algos-provided}

In v\ver, the NIMBLE algorithm library is quite limited beyond MCMC.
It includes some basic utilities for calculating and simulating sets
of nodes.  And it includes a couple of algorithms, particle filters
and MCEM, that illustrate the
kind of programming with models that can be done with NIMBLE.

\section{Basic Utilities}
\label{sec:basic-utils}

\subsection{\cd{simNodes}, \cd{calcNodes}, and \cd{getLogProbs}}
\label{sec:cdsimn-cdcalcn-cdget}



  \cd{simNodes}, \cd{calcNodes} and \cd{getLogProb} are basic nimbleFunctions that simulate, calculate, or get the log probabilities
  (densities), respectively, of the same set of nodes each time they
  are called.  Each of these 
takes a model and a character string of node names 
  as inputs. If \cd{nodes} is left blank, then all the nodes of the model
  are used. 
  
  For \cd{simNodes}, the nodes provided are topologically sorted to 
  simulate in the correct order. For \cd{calcNodes} and \cd{getLogProb},
  the nodes are sorted and dependent nodes are included.  Recall that
 the calculations must be up to date (from a calculate call) for \cd{getLogProb} 
  to return the values you are probably looking for.
  
<<Basic_Utils_Algs>>=
simpleModelCode <- nimbleCode({
  for(i in 1:4){
    x[i] ~ dnorm(0,1)
    y[i] ~ dnorm(x[i], 1) #y depends on x
    z[i] ~ dnorm(y[i], 1) #z depends on y
    #z conditionally independent of x
  }
})

simpleModel <- nimbleModel(simpleModelCode)
cSimpleModel <- compileNimble(simpleModel)

#simulates all the x's and y's
rSimXY <- simNodes(simpleModel, nodes = c('x', 'y') ) 

#calls calculate on x and its dependents (y, but not z)
rCalcXDep <- calcNodes(simpleModel, nodes = 'x')

#calls getLogProb on x's and y's
rGetLogProbXDep <- getLogProbNodes(simpleModel,
                  nodes = 'x')

#compiling the functions
cSimXY <- compileNimble(rSimXY, project = simpleModel)
cCalcXDep <- compileNimble(rCalcXDep, project = simpleModel)
cGetLogProbXDep <- compileNimble(rGetLogProbXDep,
                           project = simpleModel)

cSimpleModel$x
cSimpleModel$y
#simulating x and y
cSimXY$run()
cSimpleModel$x
cSimpleModel$y

cCalcXDep$run()

#Gives correct answer because logProbs
#updated by 'calculate' after simulation
cGetLogProbXDep$run()

cSimXY$run()

#Gives old answer because logProbs
#not updated after 'simulate'
cGetLogProbXDep$run()
cCalcXDep$run()
@


\subsection{\cd{simNodesMV}, \cd{calcNodesMV}, and \cd{getLogProbsMV}}

There is a similar trio of nimbleFunctions that does each job
repeatedly for different rows of a modelValues object.  For example,
\cd{simNodesMV} will simulate in the model multiple times and record
each simulation in a row of its modelValues.  \cd{calcNodesMV} and
\cd{getLogProbsMV} iterate over the rows of a modelValues, copy the
nodes into the model, and then do their job of calculating or
collecting log probabilities (densities), respectively.  Each of these
returns a numeric vector with the summed log probabilities of the
chosen nodes from each
each row. \cd{calcNodesMV} will 
  save the log probabilities back into the modelValues object if
  \cd{saveLP == TRUE}, a run-time argument. 

Here are some examples:

<<Basic_Utils_MV>>=
mv <- modelValues(simpleModel)
rSimManyXY <- simNodesMV(simpleModel, nodes = c('x', 'y'), mv = mv)
rCalcManyXDeps <- calcNodesMV(simpleModel, nodes = 'x', mv = mv)
rGetLogProbMany <- getLogProbNodesMV(simpleModel,
                    nodes = 'x', mv = mv)

cSimManyXY <- compileNimble(rSimManyXY, project = simpleModel)
cCalcManyXDeps <- compileNimble(rCalcManyXDeps, project = simpleModel)
cGetLogProbMany <- compileNimble(rGetLogProbMany, project = simpleModel)

cSimManyXY$run(m = 5) # simulating 5 times
cCalcManyXDeps$run(saveLP = TRUE) # calculating 
cGetLogProbMany$run() #
@
  
  

\section{Particle Filter}

  NIMBLE includes an algorithm for a basic particle filter to be used for approximating the
  log likelihood of a state-space model.  A particle filter can be built
  around such a model by a call to \cd{buildPF}. This nimbleFunction requires
  setup arguments \cd{model} and \cd{nodes}, which is a character vector specifying latent model nodes.
  The particle filter can be run by specifying the number of particles.
  
  Here is an example, using a linear state-space model for which we
  can also calculate the likelihood using the Kalman Filter to
  verify if the particle filter seems to be working.
  
<<particle_Filter_Chunk>>=
# Building a simple linear state-space model. 
# x is latent space, y is observed data
timeModelCode <- nimbleCode({
  x[1] ~ dnorm(mu_0, 1)
	y[1] ~ dnorm(x[1], 1)
	for(i in 2:t){
		x[i] ~ dnorm(x[i-1] * a + b, 1)
		y[i] ~ dnorm(x[i] * c, 1)
	}

	a ~ dunif(0, 1)
	b ~ dnorm(0, 1)
	c ~ dnorm(1,1)
	mu_0 ~ dnorm(0, 1)
})

#simulate some data
t = 25; mu_0 = 1
x = rnorm(1 ,mu_0, 1)
y = rnorm(1, x, 1)
a = 0.5; b = 1; c = 1
for(i in 2:t){
	x[i] = rnorm(1, x[i-1] * a + b, 1)
	y[i] = rnorm(1, x[i] * c, 1)
}
## build and compile the model
rTimeModel <- nimbleModel(timeModelCode, constants = list(t = t), data = list(y = y) )
cTimeModel <- compileNimble(rTimeModel)

#Build the particle filter
rPF <- buildPF(rTimeModel, "x")
cPF = compileNimble(rPF,project = rTimeModel)

#Set parameter values
cTimeModel$mu_0 = 1
cTimeModel$a = 0.5
cTimeModel$b = 1
cTimeModel$c = 1
cTimeModel$mu_0 = 1

#Run particle filter with 
#5000 particles
cPF$run(m = 5000)
@

\section{Monte Carlo Expectation Maximization (MCEM)}

   Suppose we have a model with missing data (or a layer of latent
  variables that can be treated as missing data) and we would like to
  maximize the marginal likelihood of the model, integrating over the
  missing data. A brute-force method for doing this is MCEM. This is
  an EM algorithm in which the missing data are simulated via Monte
  Carlo (often MCMC, when the full conditional distributions cannot be
  directly sampled from) at each iteration.  MCEM can be slow, and
  there are other methods for maximizing marginal likelihoods that can
  be implemented in NIMBLE.  The reason we started with MCEM is to
  explore the flexibility of NIMBLE and illustrate the combination of
  R and NIMBLE involved, with R managing the highest-level processing
  of the algorithm and calling nimbleFunctions for computations.
  
  We will revisit the \nm{pump} example to illustrate the use of
  NIMBLE's MCEM algorithm.
  
  
<<build-MCEM>>=
pumpMCEM <- buildMCEM(model = newPump,
                      latentNodes = 'theta',
                      burnIn = 100,
                      mcmcControl = list(adaptInterval = 20),
                      boxConstraints = list( list( c('alpha', 'beta'), 
                          limits = c(0, Inf) ) ), 
                      buffer = 1e-6)
@

%  When building the MCEM algorithm, the arguments supplied are \cd{model}, \cd{latentNodes}, \cd{burnIn}, \cd{mcmcControl}, \cd{boxConstraints} and \cd{buffer}. 
  The first argument, \cd{model}, is a NIMBLE model, which can be
  either the uncompiled or compiled version. At the moment, the model provided cannot be part of another MCMC sampler.
 
  The \cd{latentNodes} argument should indicate the nodes that will be
  integrated over (sampled via MCMC), rather than
  maximized.  These
  nodes must be stochastic, not deterministic! \cd{latentNodes} will
  be expanded as described in Section \ref{sec:arbitr-coll-nodes}:
  e.g., either \cd{latentNodes = `x'} or \cd{latentNodes = c(`x[1]',
    `x[2]')} will treat \cd{x[1]} and \cd{x[2]} as latent nodes if
  \cd{x} is a vector of two values. All other non-data nodes will be
  maximized over. Note that \cd{latentNodes} can include discrete nodes,
  but the nodes to be maximized cannot.  

 The \cd{burnIn} argument indicates the number of samples from the MCMC for the E-step that should be discarded when computing the expected likelihood in the M-step. Note that \cd{burnIn} can be set to values lower than in standard MCMC computations, as each iteration will start off where the last left off. 
  
The  \cd{mcmcControl} argument will be passed to \cd{configureMCMC} to define the MCMC to be used.

 The MCEM algorithm allows for box constraints on the nodes that will
 be optimized, specified via the \cd{boxConstraints} argument. This is
 highly recommended for nodes that have zero density on parts of the
 real line\footnote{Currently NIMBLE is not able to determine this automatically.}
 Each constraint given should be a list
 in which the first element is the names of the nodes or variables
 that the constraint will be applied to and the second element is a
 vector of length 2, in which the first value is the lower limit and
 the second is the upper limit.  Values of \cd{Inf} and \cd{-Inf} are allowed. If a node is not listed, it will be assumed that there are no constraints. These arguments are passed as \cd{lower} and \cd{upper} to R's \cd{optim} function, using \cd{method = `L-BFGS-B'}) 

 The value of the  \cd{buffer} argument shrinks the
 \cd{boxConstraints} by this amount.  This can help protect against
 non-finite values occuring when a parameter is on its boundary value. 
  
    Once the MCEM has been built for the model of interest using \cd{buildMCEM}, it can be run as follows.

 <<run-MCEM>>=
pumpMCEM(maxit = 20, m1 = 250, m2 = 500)

pumpMCEM(maxit = 50, m1 = 1000, m2 = 5000)
@

There are three run-time arguments:
 
The \cd{maxit} argument is the number of total iterations to run the algorithm. More advanced MCEM algorithms have a stopping criteria based on computing the MCMC error. Our current draft implementation of the algorithm merely runs \cd{maxit} iterations and then terminates. 
  
  Halfway through the algorithm, the sample size used for the E-step
  switches from \cd{m1} to \cd{m2}.  This provides smaller MCMC error
  as the algorithm converges. If \cd{m1} or \cd{m2} is less than or equal to \cd{burnIn} as defined in \cd{build\_MCEM}, the MCEM algorithm will immediately quit.

  When using the MCEM algorithm, we suggest first starting with small values of \cd{m1} and \cd{m2} to get an estimate of how long the algorithm will take for larger MCMC samples. The speed of the algorithm will be linear in \cd{m2} (assuming that $m2 > m1$ as intended).

